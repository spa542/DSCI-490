{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d82d3c",
   "metadata": {},
   "source": [
    "# LSTM Model Prediction on Bitcoin using Tashman Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd3a03",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce4a44d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988dcf31",
   "metadata": {},
   "source": [
    "## Defining Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6738d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Classes and Functions\n",
    "def split_sequences(input_sequences, output_sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list() # instantiate X and y\n",
    "    for i in range(len(input_sequences)):\n",
    "        # find the end of the input, output sequence\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out - 1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(input_sequences): break\n",
    "        # gather input and output of the pattern\n",
    "        seq_x, seq_y = input_sequences[i:end_ix], output_sequence[end_ix-1:out_end_ix, -1]\n",
    "        X.append(seq_x), y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes # output size\n",
    "        self.num_layers = num_layers # number of recurrent layers in the lstm\n",
    "        self.input_size = input_size # input size\n",
    "        self.hidden_size = hidden_size # neurons in each lstm layer\n",
    "        # LSTM model\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=0.2) # lstm\n",
    "        self.fc_1 = torch.nn.Linear(hidden_size, 128) # fully connected \n",
    "        self.fc_2 = torch.nn.Linear(128, num_classes) # fully connected last layer\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # hidden state\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        # cell state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        # propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) # (input, hidden, and internal state)\n",
    "        hn = hn.view(-1, self.hidden_size) # reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) # first dense\n",
    "        out = self.relu(out) # relu\n",
    "        out = self.fc_2(out) # final output\n",
    "        return out\n",
    "    \n",
    "def training_loop(n_epochs, lstm, optimiser, loss_fn, X_train, y_train, X_test, y_test):\n",
    "    for epoch in range(n_epochs):\n",
    "        lstm.train()\n",
    "        outputs = lstm.forward(X_train) # forward pass\n",
    "        optimiser.zero_grad() # calculate the gradient, manually setting to 0\n",
    "        # obtain the loss function\n",
    "        loss = loss_fn(outputs, y_train)\n",
    "        loss.backward() # calculates the loss of the loss function\n",
    "        optimiser.step() # improve from loss, i.e backprop\n",
    "        # test loss\n",
    "        lstm.eval()\n",
    "        test_preds = lstm(X_test)\n",
    "        test_loss = loss_fn(test_preds, y_test)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: %d, train loss: %1.5f, test loss: %1.5f\" % (epoch, loss.item(), test_loss.item()))\n",
    "            \n",
    "def plot_big_picture_time_series_predictions(true, preds, train_test_cutoff):\n",
    "    plt.figure(figsize=(10,6)) # plotting\n",
    "    plt.axvline(x=train_test_cutoff, c='r', linestyle='--') # size of the training set\n",
    "    plt.plot(true[::-1], label='Actual Data') # actual plot\n",
    "    plt.plot(preds[::-1], label='Predicted Data') # predicted plot\n",
    "    plt.title('Time-Series Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_small_picture_predicted_days(test_target, test_predict):\n",
    "    plt.plot(test_target, label=\"Actual Data\")\n",
    "    plt.plot(test_predict, label=\"LSTM Predictions\")\n",
    "    plt.title('Actual Data vs. LSTM Predictions')\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "    \n",
    "def tashman_testing(lstm, loss_fn, X_test, y_test, mm):\n",
    "    # test loss\n",
    "    lstm.eval()\n",
    "    test_preds = lstm(X_test[-1].unsqueeze(0))\n",
    "    test_preds = test_preds.detach().numpy()\n",
    "    test_preds = mm.inverse_transform(test_preds)\n",
    "    test_preds = test_preds[0].tolist()\n",
    "    \n",
    "    test_target = y_test[-1].detach().numpy() # last sample again\n",
    "    test_target = mm.inverse_transform(test_target.reshape(1, -1))\n",
    "    test_target = test_target[0].tolist()\n",
    "    \n",
    "    plot_small_picture_predicted_days(test_target, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee1d75",
   "metadata": {},
   "source": [
    "## Reading in, Viewing, and Refactoring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc7dad3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABAA0lEQVR4nO3dd3ib5bn48e8teSWOnb03EBKSEEZM2BRIgbBpC22gZbThpKWc0p5T2kJ3fxAKtKe0tEBLoeyVUigbCmGvQAIESEiIs53hONsZHpLu3x/vI1uSZVmyNSzn/lyXL7169I7ntZP31rNFVTHGGGPay5frDBhjjMlvFkiMMcZ0iAUSY4wxHWKBxBhjTIdYIDHGGNMhFkiMMcZ0iAUS0ymJyF9F5BcZOO9zInJxus/byrV2isg+2bhWZyAiI9w9+3OdF5NdYuNITC6IyEpgIBAEGoG3ge+o6po4+x4P3K+qw7KYxRZi8rwLeBb4nqruzHI+LgF+COwL7AAeB65W1W0Zvu6xwHPht0B3vN9D2HhVXZ3JPJjOyUokJpfOVNUewGCgGvhzjvOTjHCeDwUOA34eu4OIFGTq4iLyQ+AG4EdAT+AIYCTwoogUpflaUfehqm+oag93/xNccq9wmgWRvZcFEpNzqloHPAqMD6eJyN0icq2IlOJ9Cx7iqk12isgQEfGLyE9FZJmI1IrIfBEZ7o49SkTeF5Ht7vWoiPO+KiKXuu1LRORNEfm9iGwVkRUicmqSeV7r8jXRnUtF5HIRWQosjUjbz213E5H/E5FVLl9vikg399kRIvK2iGwTkQWuBNaCiJQDv8ErBT2vqo2quhL4Kl4w+Yb73ewRkT4Rxx0iIptEpNC9/5aIfObu+QURGRmxb4v7SJaIjHLHF7j3r7q/4dvu7/aUiPQVkQdEZIf724yKOH6ciLwoIltEZImIfDWV65vcsUBick5EugNfA96N/UxVdwGnAusivvmuA/4XOB84DSgHvgXsdg/QZ4Cbgb7AH4BnRKRvK5c/HFgC9ANuBO4UEUkiz8PdtT+MSD7HnW98nEN+D0wGjgL6AD8GQiIy1OX3Wpd+JfAvEekf5xxHASXAY5GJrmrtOeAk97t5B/hKxC4XAI+qaqOInAP8FPgy0B94A3go5jqJ7iNV04ELgaF4VXHvAHfh3etnwK8A3BeGF4EHgQF4f9tbRWRCnHOaTsYCicmlf4vINrx6/pOA36Vw7KXAz1V1iXoWqOpm4HRgqarep6oBVX0IWAyc2cp5Vqnq31U1CNyDV802MIk8vwm8BlwX8dlvVXWLqu6JPEBEfHiB7vuqulZVg6r6tqrWA98AnlXVZ1U1pKovAvPwglSsfsAmVQ3E+Wy9+xy8h/H57tqC9zB/0H32bZfPz9x5rgMOjiyVtHYf7XSXqi5T1e14wW6Zqr7krv1P4BC33xnASlW9y/3dPgD+BZybhjyYDLNAYnLpHFXtBRQD/w28JiKDkjx2OLAsTvoQYFVM2iq8b8TxbAhvqOput9kjwXXPUdVeqjpSVb8b87Bt0VHA6YdXkoiX35HAea5aa5sLUsfgBbRYm4B+rbTBDHafg1dNeKSIDAGOAxSv5BG+3p8irrUFr+E88vfT2n20R3XE9p4478O/65HA4TG/h68Dyf57MDlkgcTknPuG/hheb6hj4u0SJ20NXlVJrHV4D6VII4C1HcpkclrrArkJqCN+ftcA97ngFP4pVdXr4+z7DlCPVy3VxFULnQrMAXC9t/6D13ZyAfCQNnfPXAN8O+Z63VT17STuI5PWAK/F5KuHql6Wg7yYFFkgMTknnrOB3nj15rGqgb4i0jMi7Q7gGhEZ446f5NpBngX2F5ELRKRARL6GV9f/dKbvozWqGgL+AfwhoqPAkSJSDNwPnCkip7j0EhE5XkRadHV21UO/Af4sItNEpNA1Vv8TqALui9j9QeAivLaSByPS/wpcHW57EJGeInJe+u86ZU/j/d0udPdVKCKHicgBuc6YaZsFEpNLT4nITrw2klnAxaq6MHYnVV2M1yC83FV7DMFrRJ+N9817B3An0M21k5yBN85iM16j9hmquin2vFl2JfAJ8D5eddINgM+NmzkbrwG8Bu+b+Y9o5f+mqt7o9v093n3PdcdMdW0uYU8CY4BqVV0Qcfzj7toPi8gO4FO80kxOqWotcDJee846vCrHG/CqPU0nZwMSjTHGdIiVSIwxxnSIBRJjjDEdYoHEGGNMh1ggMcYY0yGZnFxuLPBIRNI+wC+Be136KGAl8FVV3eqOuRqYgTee4ApVfcGlTwbuBrrhde/8vqqq6z55L97UE5uBr7m5h1rVr18/HTVqVDpu0Rhj9hrz58/fpKrxpu7JTq8t8dYnWIs3f8/lwBZVvV5ErgJ6q+pPRGQ8XhfPKXijk18C9lfVoIi8B3wfby6mZ4GbVfU5EfkuMElVvyMi04EvqerXEuWloqJC582bl6lbNcaYLklE5qtqRbzPslW1NRVvjp1VeH3m73Hp9+BNEIdLf1hV61V1BVAJTBGRwUC5qr7jRufeG3NM+FyPAlOTmXDPGGNM+mQrkEyneYbRgaq6HsC9DnDpQ4me46fKpQ1127HpUce4SeC24834GkVEZorIPBGZV1NTk5YbMsYY48l4IBFvsZ2z8KZxSLhrnDRNkJ7omOgE1dtVtUJVK/r3j1vFZ4wxpp2yUSI5FfhAVcOzfla76irc60aXXoU3o2vYMLypEqrcdmx61DFuRtSeeNNPGGOMyZJsBJLziV4450ngYrd9MfBERPp0ESkWkdF48wS956q/asVbRU7wJqJ7Is65zgVeVpvzxRhjsipj3X+haeW7k/AW0wm7HpgtIjOA1cB5AKq6UERmA4uAAHC5W2wI4DKau/8+537Am6jvPhGpxCuJTM/k/RhjjGlpr5u00br/GmNM6jpD919jjDFp0BAIMfv9NYRCnacQkNGqLWOMMen19zeW87sXluD3CV+Z3GL9s5ywEokxxuSRrbsaANi8q76NPbPHAokxxuSRAr/32G4Mdp6qLQskxhiTRwr93jjsQCcKJNZGYowxeeDrd7zLqL6lDCgrASAQCuU4R82sRGKMMXngrcrNPDB3NQWuRDJ73po2jsgeCyTGGJNHwlVb1TvqaQx2jlKJBRJjjMkjBb7mx3awk4wlsUBijDF5JFwiAQhYIDHGGJOqyFJIsJP03LJAYowxnVzknIiRpZDO0nPLAokxxnRAQyDEDx7+kFWbd2XsGpGDD4Oh+EEllyyQGGNMB3y4eiv//mgdX/jdqxm7Rn0g2LT92+cWN21bIDHGmC6gvFthxq9RH4hfhWVtJMYY0wX4RNreqYMaWgkkjdZGYowx+S8bYzlaLZFY1ZYxxuS/UBZWmd2xpzFuemeZuNECiTHGdEA2SgX//dAHcdOt+68xxuSh1Zt3c/T1L7N++x4AglkokazZsidu+l7Ra0tEeonIoyKyWEQ+E5EjRaSPiLwoIkvda++I/a8WkUoRWSIip0SkTxaRT9xnN4t4rVsiUiwij7j0uSIyKpP3Y4wx989dxdpte3jio3VAbtsp9pY2kj8Bz6vqOOAg4DPgKmCOqo4B5rj3iMh4YDowAZgG3Coifnee24CZwBj3M82lzwC2qup+wE3ADRm+H2PMXi7cSStcEMnGw/zCI0ZGvb/5/EMA2LY7fttJtmUskIhIOXAccCeAqjao6jbgbOAet9s9wDlu+2zgYVWtV9UVQCUwRUQGA+Wq+o568wTcG3NM+FyPAlPDpRVjjMkEwXvEhBvZW+tRlU6lxQUU+Zsf19td4/uvn1yY8WsnI5Mlkn2AGuAuEflQRO4QkVJgoKquB3CvA9z+Q4HIlVqqXNpQtx2bHnWMqgaA7UDf2IyIyEwRmSci82pqatJ1f8aYvVDsV9X73lmZ8WuqKhGzx3P0vt5jrqykcyxym8lAUgAcCtymqocAu3DVWK2IV5LQBOmJjolOUL1dVStUtaJ///6Jc22MMQn4mqq2vEfNoJ7e0rfD+3TL2DWDIY0a+NivrBiAxRtqM3bNVGQykFQBVao6171/FC+wVLvqKtzrxoj9h0ccPwxY59KHxUmPOkZECoCewJa034kxxjjhqq09jUE276zn/ndXZ/yaQVX8EYEkcrszrJKYsUCiqhuANSIy1iVNBRYBTwIXu7SLgSfc9pPAdNcTazReo/p7rvqrVkSOcO0fF8UcEz7XucDLqlnoi2eM2WuFn+G3vLKMyde+1JQeHhx44Z1zufqxj9N6TVXw+SICScT2T/6V3mu1R6Yr2L4HPCAiRcBy4Jt4wWu2iMwAVgPnAajqQhGZjRdsAsDlqhqe8vIy4G6gG/Cc+wGvIf8+EanEK4lMz/D9GGP2cq315wmP6Xhj6SYAfvvlSWm7ple11fw+MpC8uLA6bddpr4wGElX9CKiI89HUVvafBcyKkz4PmBgnvQ4XiIwxJht8rfQLDWSwiimoit8n/OKM8YwdWBZdzeXPfUfVztHkb4wxeeLJj9bFTQ+ElJ89/klGrqnqNbbPOGZ0i8+27W5EVVstKWWDBRJjjEnCTx79mBPG9Wf5pvgrIQaCygNz09/wHgopD723JuE+exqDdC/K3ePc5toyxpgkPDJvDd+5P/7kiZC5Ee4NSVSZPffJhoxcO1kWSIwxJgWnHTgobnrsIlPp6kCaTID6vDq340kskBhjTBsiG9Jbiw+x6ctq4leBpXztVgLJvv1Lm7ZH9i2Nu0+2WCAxxpg2RM6nlexCVsUF6Xm8tlYiuWLqmKbtXA9KtEBijDFtWBHRwJ5sU0hBmrrlhhevmji0PCr97IOHNm3nejp5CyTGGNOG8Df+vqVFLcaLLL5mGjee23LwYbrm2Hhn2Wag5VTykbKx3G8iFkiMMaYNkV/4G4NK39KipvfFBT6mjOoT55j0PNy///BHQOsj6sFKJMYY0+lFBoWGQIjuxf6m9yISNWVJWLoLCU8tiD8QErKz3G8iFkiMMaYN4W/8It64jtKYwX/x2kPS/Wy/9Nh9Ws9f0AKJMcZ0auESiapXIikvKYz6vMDX8lGqLZdG6pD9BvRo9TMrkRhjTCcXOdawMRiib4+iqM8L4lRtpbvZole3wlY/C1kbiTHGdG6R3/gbgiGKYsaIlBT6Yw9JW2P7yL7d6Vbop7S45Vxat184uUX+csECiTHGtCEyKDQGQhT6ox+dkYMPrzl7ApC+NpLe3YuoGNU77mcnTxhEUYGPXC+SaIHEGGPaEFl1FK9EEr16ofdZe+fauv/dVdz04udN74MhjVt11nQ9EYIhG9lujDGdWuQ4jYZAiCJ/64/O8EO/vQWSn//7U/40Z2nUtf1xGvPD/D7JeYnE1iMxxpg2/PW1ZYAXRAIhbVEiiRQunaSrjaTNEolPcj6y3QKJMca04YPV2wCorQ/g9wmFCebRaiqRpOnZHgiFEi6n65VIrLHdGGM6tRPHDQBg7MAygiGlyN+yl1ZYeHBiOksk/gTTo/hEWp1qPlsskBhjTBv2H1gG0FSlVVggPHjp4fxp+sEt9g0/9NNXImmrasvrDBAKKc9/uoHausb0XDgFGQ0kIrJSRD4RkY9EZJ5L6yMiL4rIUvfaO2L/q0WkUkSWiMgpEemT3XkqReRmcbOXiUixiDzi0ueKyKhM3o8xZu8UnvF3T2MQgCK/j6P26xc1lXuYP81VW6GQxp3LK6x6Rz2PzFvDnMUb+c798znw1/9Jz4VTkI0SyQmqerCqVrj3VwFzVHUMMMe9R0TGA9OBCcA04FYRCZcfbwNmAmPczzSXPgPYqqr7ATcBN2Thfowxe5nwNPKVG3cCJGxs96e5sT0Q0qTWNnllyca0XK89clG1dTZwj9u+BzgnIv1hVa1X1RVAJTBFRAYD5ar6jnods++NOSZ8rkeBqZJormVjjGmHhphJEWMHJEbyuUdQ9Y66lK9T50o8kYJtlEjCHpy72uUt+4/ATAcSBf4jIvNFZKZLG6iq6wHc6wCXPhRYE3FslUsb6rZj06OOUdUAsB3oG5sJEZkpIvNEZF5NTU1abswYs/eIXcwq0TiSjbVeAPnZvz9N6RqfV9dyyyuVLa/dRmN7rHhTqWRapq94tKquE5EBwIsisjjBvvF+U5ogPdEx0QmqtwO3A1RUVOS2e4MxJu/ErolemKBqKzzvVklhat/TT77p9bjpbQ1IjJWLKpmMlkhUdZ173Qg8DkwBql11Fe41XLFXBQyPOHwYsM6lD4uTHnWMiBQAPYEtmbgXY8zeqzGmaitRieSsg4YAcMakIR2+rqpS1xikOEFQGtqrW9R7Xw5q9zMWSESkVETKwtvAycCnwJPAxW63i4En3PaTwHTXE2s0XqP6e676q1ZEjnDtHxfFHBM+17nAy9reCW6MMaYVz3yyPur9wPLiFvvM+eEX+MNXD0JEKCn0pWVq97pGbyR9WUnrlUe3fP3QqPe5mAk4k1VbA4HHXdt3AfCgqj4vIu8Ds0VkBrAaOA9AVReKyGxgERAALlfVcMvTZcDdQDfgOfcDcCdwn4hU4pVEpmfwfowxBoCxg8papO3bvwf79vcWnyrw+VIabR7v+++oq57hJ9PGAVBW0vpaJLFjTCLXk8+WjAUSVV0OHBQnfTMwtZVjZgGz4qTPAybGSa/DBSJjjMmEBWu2Rb0/fmx/uhclfnT6famNNn/t8/idgG543mtW/mj1Ni48YmTcfWK7Bi+r2UVjsOVU95lkI9uNMSaBV5dEP+T7dG/7G3+q819dctf7CT/vU5p8iQTgN08tTPra6WCBxBhjEujVPfohnsxAw1RLJG256MhRrX7Wo7hlkGmthJMpFkiMMSaBgeUlAAzr7fWOSiY+FPgkLY3t/cu8Rv1EI+n79WhZQlqzZU+Hr50KCyTGGJPAh6u3As3deXfVB9o8pqjAx+44o9STEQ4e0DxfV6IuvQV+H7mez8MCiTHGJPC315cDzd/8t+9pe3bdQeUlbNieXKngsQ+aJ+648SuTiGzyaAh4wSjR7L8A//7u0UldK1MskBhjTBL69fBKCskEkqICX9JtJHe9tbJp2+8TDhrWq+l9fcAbUe9rI5BMGFLOmQcN4fHvHgXAuDjdkzPJAokxxiShvJvX5TeZQFKQQq+tHhFzYwVV+fYX9ml6Hw4kbZVICvw+/nz+IRwyojfTJgxK2xT2ybJAYowxSSh3gwKTaSPx+3wEgsk9zUf06d60HQhq3Mb8ZGb/DSsu9FEfaF/7THvZmu3GGJOEkkI/3zp6NGccNLjNfVMpkURO7hgIhahvDLXYJ6VAUuBrKslki5VIjDEmgTEDvGlPxg8u55dnjufQEb3bOAL8fiEQSu5hfs87q5q2JwzpyeH79OH4sf2jz5dCt6ziAn+LQDJ/1Raqtu5O+hypskBijDEJDO/TnQOH9myzwTtSKiWSsB+dMpbJI3tT6Pdx9zenRH2WyrWLC3zUR3Q9VlW+cts7nPHnN1PKTyoskBhjTAIh1ZTHabRnZPvlJ+yX2kVaUVzoY1dDkMOvewmAjbX1AGzb3UhtXdsdBdrDAokxxiSgCqmu4F3gk6Qb29OtuMBbWKt6hxdANu9saPrs3x+uzcg1rbHdGGMSCKmmvOpgj+JCdibRuwtg0rCe9E5iIshkRTbeV23dzZ6Iaq5E09F3hJVIjDGmDSk0UQDQs5sXSGLXeo+nIRCiOMFcWqkKl0gA/jV/LXVRgSQzZQcLJMYYk4DXRpJaJCks8PZPpp2kIRBKOCljqiKD0sJ122MCiZVIjDEm61RTL5GEJ1lsa4T5V//6Dss37UpvIImo2vrPouqoqq2SBGu/d4QFEmOMScBrI0ktkoQDT1trl7y3cgtAWqu2IvM6ok93dtd7gaRvaVHTUsDpZoHEGGMS8HptpXZMuESSzCJYAEVpXBZXab5maXEBP3nsYwCe+/6xlBZnpo3Eem0ZY0wCquBr53M+2aEk6azaOnPSEHp1K+KR99ewfNNODh3Rm/mrttK3R3HbB7dTxkskIuIXkQ9F5Gn3vo+IvCgiS91r74h9rxaRShFZIiKnRKRPFpFP3Gc3i2v5EpFiEXnEpc8VkVGZvh9jTNdWvaOOzTvrm94rmnBhqXjCo9rDXYBVFU1QOkkUSM46aEhK1y7w+zhh3AAK/N5YlvGDy+ndvTCl+bpSlY2qre8Dn0W8vwqYo6pjgDnuPSIyHpgOTACmAbeKSLgf223ATGCM+5nm0mcAW1V1P+Am4IbM3ooxpqs7/Lo5TL72pab3oXZUbT3uBv7933+WADD66me55K73W90/sstu2PFj+zO0VzduPv+Q1C7uFPq9NVGCqhkNIpDhQCIiw4DTgTsiks8G7nHb9wDnRKQ/rKr1qroCqASmiMhgoFxV31EvpN8bc0z4XI8CUyXVfnrGGJOAavtLJJFdb1/7vKbV/eM96O/+5hTeuurElK4be85AMNSu/Kcq0yWSPwI/BiJH5QxU1fUA7nWASx8KrInYr8qlDXXbselRx6hqANgO9I3NhIjMFJF5IjKvpqb1P6YxxsQKtWOKlKbG9gTjEfc0NAeZVCd4TEaBm+8rGMrjQCIiZwAbVXV+sofESdME6YmOiU5QvV1VK1S1on///nEOMcaYaOE2jSUbapNaFTFS+Ln9ydrtrS6EtSmiHSaZEfCpKvCHA0lq65m0RyZLJEcDZ4nISuBh4EQRuR+odtVVuNeNbv8qYHjE8cOAdS59WJz0qGNEpADoCWzJxM0YY/Yuv3lqEQB7GoMsWLMtpWPDD+612/Yw4VcvxN0nstor1ZmCk1Hg8xEIhgiptrvXWbIydnpVvVpVh6nqKLxG9JdV9RvAk8DFbreLgSfc9pPAdNcTazReo/p7rvqrVkSOcO0fF8UcEz7Xue4auZly0xjTpdz99sp2H5tMVVJDRCkkU1VbwZDy+IdrWbNlT9rPH3WtjJ49vuuB2SIyA1gNnAegqgtFZDawCAgAl6tqOGRfBtwNdAOecz8AdwL3iUglXklkerZuwhjTtYlAo3vYX3zkyJSOTaYmqTFimvnGDEw57/cLuxqys3Z7VgKJqr4KvOq2NwNTW9lvFjArTvo8YGKc9DpcIDLGmI6KrNAQaJqnanif7imdp7UVDUMhbfqsIWI53IHl6R8sWJjp+qwINkWKMcY4kW0VPhE27qgDUp8Lq7WqrcjqrHBp55KjRjHjmNGpZrVNmW5gj2SBxBhjnMhVDUOq/PpJr8E90RiQeFp7hjdGBJJwUDnr4CEUpHGurbBCvwUSY4zJukDEwI+QQq3rulsfSK17bs9u8df9iAxUje6c6ZywMZK/s1Vticj+IjJHRD517yeJyM8zmzVjjMmu2HXWTz9wEABXTB2T0nl6tbJ0bmMosmrLu1Y6J2yMVBBRLDp4eK+MXCMs2Tv4O3A10Aigqh9jPaSMMV1MY8xQ9PpG7/3Ivqk1tv/yzPFx06NKJK5qqzBDJZLaOm8Q5TkHD+Gxy47KyDXCkr2D7qr6XkxacivbG2NMnogdzxEefZ5q9VN5K0vartmyu2mUfLjXVqbaMnbUeY/oScN6tdqLLF2S/e1sEpF9cdOPiMi5wPqM5coYY3IgtmqreocXSNLVGP6129/l5JteA5ob2zPVRhLuypyNzlvJjiO5HLgdGCcia4EVwDcylitjjMmB2KlKalyJJJ2lhuod9Wzb3ZDxqq2mCQmzMCF6UoFEVZcDXxSRUsCnqrWZzZYxxmRf7OSJ4fmw0j2477bXltG31GuQL8xQY/uQXt0AGFCWuZURw5LttXWdiPRS1V2qWisivUXk2kxnzhhjsil2qpL6QAi/T9LexlDs9zX32spQieTSY0bztwsnM23ioIycP1Kyd3Cqqm4Lv1HVrcBpGcmRMcbkSGxje+XGne2eUPHOiyta/ezmlyvZ3eA1hmeqsb3A7+OUCYOyUrWVbCDxi0hT+UhEugGZLy8ZY0wWxXb/7YipBwxk5fWnt/r5I++vodAvWXnQZ1qygeR+YI6IzBCRbwEv0rzErTHGdAnhXlunHzg449fatLOB0uJcTMCefkkFElW9EW9W3gOACcA1Ls0YY7qM8BQpx+3fLyvXG9KzW1auk2lJh0NVjVwHxBhjupy3KjcB3uqC6fLGj09gZ32AU//0RovPuhX503adXEoYSETkTVU9RkRqiV4LXQBV1fKM5s4YY7LolleWAVAXaF4Q6rovHdihc8auZTJuUBmLN3gjKEoKu8a8uQnvQlWPca9lqloe8VNmQcQY01WFInpqhXtXpctvv9wcmEoKukaJpM1wKCK+8Ky/xhizN+gZMXtvuntVFfh8HDi0J9A8aDDftRlIVDUELBCREVnIjzHG5Mw5Bw8B4PDRfZrSIpffTYcCv/DJ2u0AVG3dndZz50qyje2DgYUi8h6wK5yoqmdlJFfGGJMDIsKIPt0JRQSPdMWRv104mdc/r2HcoLKmtNi5vfJVsoHkN6meWERKgNfxBi4WAI+q6q9EpA/wCDAKWAl81Y2UR0SuBmYAQeAKVX3BpU8G7ga6Ac8C31dVdYMk7wUmA5uBr6nqylTzaowx4D3YC3wSNZpdSc/D/pQJgzhlgjddyd8unMy375sftfRuPktYtSUiJSLyA+A8YBzwlqq+Fv5p49z1wImqehBwMDBNRI4ArgLmqOoYYI57j4iMx1ssawIwDbhVRMItUbcBM4Ex7meaS58BbFXV/YCbgBuSvXFjjIkVDHlzaw0oK2lKy0ShoY+bsLEhxSV8O6u22kjuASqAT4BTgf9L9sTq2eneFrofBc6meVT8PcA5bvts4GFVrVfVFUAlMEVEBgPlqvqOepWV98YcEz7Xo8BU6QrzDRhjciIQVPw+oajAxz8u8ebKysQyteGp4xu6SImkraqt8ap6IICI3AnErpKYkCtRzAf2A25R1bkiMlBV1wOo6noRGeB2Hwq8G3F4lUtrdNux6eFj1rhzBURkO9AX2JRKPo0xBrxJGwvcJIonjhvIgl+eTM/u8Vc77IhiN3V8Y6BrtJG0VSJpDG+oasqdqVU1qKoHA8PwShcTE+weryShCdITHRN9YpGZIjJPRObV1NS0kWtjzN4qEFL8EaPaMxFEoHkN+CumjsnI+bOtrRLJQSKyw20L0M29T2lku6puE5FX8do2qkVksCuNDAY2ut2qgOERhw0D1rn0YXHSI4+pEpECoCewJc71b8db4ZGKioqu8RXAGJN2exqCTaWFTOpeVJBwZuB809bIdn/MaPaCZEe2i0h/EenltrsBXwQWA08CF7vdLgaecNtPAtNFpFhERuM1qr/nqsFqReQI1/5xUcwx4XOdC7ys6e70bYzZK6gqS6pr2bd/j1xnJe9kcg7jwcA9rp3EB8xW1adF5B1gtojMAFbj9QhDVReKyGxgERAALlfV8IQ3l9Hc/Tdy8sg7gftEpBKvJDI9g/djjOnCGoIhtu9pZFjvrjHaPJsyFkhU9WPgkDjpm4GprRwzC2+6+tj0eUCL9hVVrcMFImOM6Yhd9d731tIuMiNvNnWNqSeNMaaDdtV7/Ym6d5HFprLJAokxxgA7XSDpYYEkZRZIjDGG5hJJV1n+NpsskBhjDJElEmsjSZUFEmOMoXmZ3bKSzAxC7MoskBhjDPD3N1YAUG6BJGUWSIwxBjh/ijexxqCeJW3saWJZIDHGGLwJGwdbEGkXCyTGGAPUB0JZmWerK7LfmjHGAHWNQUoKrcdWe1ggMcYYoK7RSiTtZb81Y4wB6gNBiq1E0i42hNMYs9e7443lvLu8xVJGJklWIjHG7PWufeazXGchr1kgMcYY0yEWSIwxxrng8BG5zkJeskBijNnrHT+2PwD/88X9c5yT/GSBxBiz1wuGlENG9KJ/WXGus5KXLJAYY/Z6O+sDtqBVB1ggMcbs9XbWBSgrsUDSXhZIjDF7vd0NNj1KR2QskIjIcBF5RUQ+E5GFIvJ9l95HRF4UkaXutXfEMVeLSKWILBGRUyLSJ4vIJ+6zm0VEXHqxiDzi0ueKyKhM3Y8xputqDNr0KB2Ryd9cAPihqh4AHAFcLiLjgauAOao6Bpjj3uM+mw5MAKYBt4pI+CvCbcBMYIz7mebSZwBbVXU/4CbghgzejzGmi2oMhij0WyBpr4z95lR1vap+4LZrgc+AocDZwD1ut3uAc9z22cDDqlqvqiuASmCKiAwGylX1HVVV4N6YY8LnehSYGi6tGGNMshqDaoGkA7Lym3NVTocAc4GBqroevGADDHC7DQXWRBxW5dKGuu3Y9KhjVDUAbAf6xrn+TBGZJyLzampq0nRXxph8d/kDH/Dqko00WImkQzL+mxORHsC/gB+o6o5Eu8ZJ0wTpiY6JTlC9XVUrVLWif//+bWXZ5JEbnl/MfxZuyHU2TB7atLOeZz5ZzyV3vU9DIESRtZG0W0Z/cyJSiBdEHlDVx1xytauuwr1udOlVwPCIw4cB61z6sDjpUceISAHQE7ApPPcit726jJn3zc91Nkweqrj2paj3h47olZuMdAGZ7LUlwJ3AZ6r6h4iPngQudtsXA09EpE93PbFG4zWqv+eqv2pF5Ah3zotijgmf61zgZdeOYowxrVpWs7NF2n4DeuQgJ11DJkfgHA1cCHwiIh+5tJ8C1wOzRWQGsBo4D0BVF4rIbGARXo+vy1U16I67DLgb6AY8537AC1T3iUglXklkegbvx3Qy9p3BtNcrize2SOvVvSgHOekaMhZIVPVN4rdhAExt5ZhZwKw46fOAiXHS63CByOx9GoKhXGfB5Kl4DeulRTYgsb1sTgCTt+oDFkhM+3ywemvT9qwvTaTI78NGDrSfdVMweauuMRj1/q3KTS3SjIln3bY9TduHDO/NeRXDE+xt2mKBxOStqb9/rWn7/ZVb+Podc/njS0tzmCOTL2rrAk3bg3qW5DAnXYNVbZm89Hl1LbX1zQ+DpdVeL5zVW3blKksmj+xqCHDWQUP4+RkH0KfUGtk7ykokJi/VN0a3j4SrtAJB68llElu0bgdrtuxhd0OAAWVWGkkHCyQmL8WOQl7r6ryDIQskJrH7564CYNXm3TnOSddhgcR0CXe+uQKAgAUSk8D67Xt4cO5qAG762sG5zUwXYoHE5KXGVsaQ7G4IxE03BuCxD9Y2bU8c2jOHOelaLJCYvNRaFdb7K7fGTTcGoMHGHmWEBRKTl6wKy7RHeF32q04dl+OcdC0WSExeCriqrZ+ddkCLzz5duz3b2TF5Ijx6/fzDRuQ4J12LBRKTl8JVW/FmtQg3vBsT6+63vX8b3YttXq10skBi8lKjCyTlJYUtPvu8ujbb2TF54Kp/fcyaLV43cVsNMb3st2nyUjDkVW2NHVTG786dFPXZhCHluchSp1ZTWx81v9Te6OH3vZW8v3zo0Db2NKmyQGLyUqMbwe73CcePHQBAn9IiyksKmD2vitq6xlxmr9M5bNZLHHX9y7nORs6s2tw8dc7/nXdQDnPSNVkgMXkp3EZS6PeheNsFPmGHm4zvnrdX5iprndo1Ty/izaWbcp2NrPtw9TYAbrngUJsuPgMskJi8FB6Q6PcJ/XsUM+OY0dw7Y0rT57Z4Ynx3vrmCb9w5t+n9NU8v4vXPa3KYo45TVcb87FlufbWy1X027awH4Jgx/bKVrb2KBRKTl8KTMxb6BRHhF2eMZ9yg5raR219fzs56G+WeyOfVtdz55gou+sd7uc5Ku4RCyi+f+JTRVz9LY1C58fkljLrqGR6dX9Vi3827Gij0C+UlNuF5JlggMXlpzVZvwr3S4vgPhtr6ALe80vwNtXJjLXM+q85K3vLF/zzyUa6z0CFzV2zh3ndWtUi/8p8Lot7/Z+EGbnt1GY1BtWqtDLHwbPLS6i27GVheTL8exa3uowofV23jrL+81ZS2dNap1vUTr2rwxHEDWLhuBwCbd9bTN8HvsjN6+uN1cdNPnTiIz6trWbttDyeMHdAUMMcNKsti7vYuGfsfJSL/EJGNIvJpRFofEXlRRJa6194Rn10tIpUiskRETolInywin7jPbhb3lUJEikXkEZc+V0RGZepeTOezZEMtI/uUtki/46KKpu2e3Qqjggg015XvTeJV8Z1+8xtR85VNvvYlQiHNq6WKH3Cz+A7uWcKy607j6e8dA3iDVE++6XW+edf7jLrqGXY1BDl5/EAe++5Rucxul5bJr2Z3A9Ni0q4C5qjqGGCOe4+IjAemAxPcMbeKSHjo6W3ATGCM+wmfcwawVVX3A24CbsjYnZhOZ9Xm3YyN8w3zi+MH8qNTxgKwJ85DsXrH3hdIFsaZMubz6p08/uHaqLST//g6437xPF/43Sts352+7tNLNtRSuTEzg0T7lhbx9lUn4vdJ02y+z36yocV+X60YTvciq4DJlIwFElV9HdgSk3w2cI/bvgc4JyL9YVWtV9UVQCUwRUQGA+Wq+o6qKnBvzDHhcz0KTBWrAN0rqCq7GgL07t5yVDvA5SfsB8DNc5ZSUhj9T7x6R13G89fZ1LhS2I3nTuLk8QOb0tdvr2Nor25N7ys3essVr9q8m4P+33/4uGpbh6+9ZstuTvnj63zxD68nNQfa7oYAG93faP32PUz74+us3x5/IGXPboWcMWlwm+0eXz98BFMPGJB65k3Ssl1ZPFBV1wO41/BfdyiwJmK/Kpc21G3Hpkcdo6oBYDvQN95FRWSmiMwTkXk1Nfnd1dHAO8s3owoFSbR1jBkQXWqJF0i27W7gsQ+qGHXVM60+tPJZTa0XSL54wEB+eeb4qM/OmDS41ePiNWSn6rIH5jdf689vttmT7vIHPmDKdXM4+vqXueKhD1m8oZYjf/syz3y8nvmrmr+XhkLK9j2N9OwW/WXi/CnDAbjxK5P46zcO5YNfnMSsLx1ojewZ1llaHeP9lTVBeqJjWiaq3q6qFapa0b9//3Zm0XQW7y7bDMCXDml9qovjx/Zn0rCebN8TXUXzyycWttj34P/3Iv872+vp8/fXu96Ejx+4wXi9uhXSq3tR1GeHjerDyutP54FLDwfggMHlrLz+dPw+ifoPtmRDLaOueobDZr2UdLCdee88Pl27Iypt7dbEx76yxPuit3bbnqi1ZS5/8AO+cts7vLG0hqOvf5nbXlsGeLMZRPrtlyex8vrT+ephw5k2cXCLz01mZDuQVLvqKtzrRpdeBQyP2G8YsM6lD4uTHnWMiBQAPWlZlWa6oC27G+jdvZDhfbq3us+rS2r4uGo7q7fs5kuHDG16UIJX3RJqZT2Tf7zV9QLJUwu8/zI+n9C90E/FyN7MPG4fKkb2bqryqRjVm4uOHMkf3fKzo/p2Z1fEapM3v7wU8Eo3R/627alWdtQ18p9FXnfrqeMGNK3/8b+zP+LJBfF7WyXjwjvfY+22PfzuhSUAfPWw4W0cYbIh24HkSeBit30x8ERE+nTXE2s0XqP6e676q1ZEjnDtHxfFHBM+17nAy64dxXRxG7bXM7C8JOn9F63bwdH7NY9oPvbGV7jgjncBWBpnpuCuMJDxlcUb+cYdc3l1ifddrcyNt/H5hEcvO4qfnnYAj152VFOVT3GBn/939sSmDgxlJYXU1gWaenLt279H1Pmf/WR9q9c+5abXmfTr/wDQu3shPzv9AM452Cs9Lly3gyse+jDucdc+vahF2onjBvD6j07gmSuOiUrv16OYP00/2BrQO4mM/RVE5CHgeKCfiFQBvwKuB2aLyAxgNXAegKouFJHZwCIgAFyuquEuN5fh9QDrBjznfgDuBO4TkUq8ksj0TN2L6TxUlUXrtrN/G2MCZn1pIj973Ot5XlnjNSKfNH4gL7pvyeGG5b+4QYsTh5bj9/lYsGYbE3/1As9ecSzjW5lFeMGabSyo2saFR4zstHXv37z7fQDerPTm1br7W4eldHx5t0LeqtzElOteYtPOBk6dOIiykgIuPGIkt766jO8+8AHLrzsNn6/l/S+JCM6/PmsC+/TvQTLf8e6IWEemYmRv5q3ayreP24cRfb2S59JZpyIk1zZmsitjgURVz2/lo6mt7D8LmBUnfR4wMU56HS4Qmb3H315fzrrtdazbnrj31fDezdVe133J++fzgy+OaQokm3Y20BgM8cRHXjXLo985ig9Xb+P8v3sllScWrG01kJx9izc2ZfXm3fz8jPFx98mVz6trOfmm11ukjx/cM6XzhEJKMKRs2tkAwHOfel1qfzxtHAvX7eC1z2tYVrOTMQOjA/p3IxrXAQ4d4Q0VExFe+MFxnH3Lm9Q1hgiFtEUQOnq/viyv2cVt35jMmAE9WsxaYANJOy/7y5i8cv1ziwG49JjRCffrVtS8At7pk4YAcMCg6MBw85ylTdslhX6O3Lcvv3CBYfH6Wk74/at8tGZbq9d46L3VKeU9G258fknc9MjfRzK+84V9W/3s56d7yxtf+c8FvLt8c9Rn4TEc504exvLrTotqxxo7qIzzp3hL3IZLSmGqyluVmxlYXsLBw3u1OvWN6ZwskJi8sGLTLkZd9UzT+7ZKAkUR3157RLQPPPf9Y7nkqFEA/Pllr1rrnm81zxo8wwWo1z6vYcWmXZxzS/TI+CUbmqtt6gMhtu7yvrGHQsr/PPIRy101Wq5EViGdO9nrp3LQ8F4pn+eYMf1Yef3p3PutKSz45clRn43q580osKBqO9Nvf5eL/vEeX/zDa01/HxH4/XkHxa32+vIhXp62ud50byyt4aePf8IjbtGpw0f3STmvJvcs7Ju8cO5tbzdt/+rMtquTAq5X1r79o6dROWBwOeccMpS7I9YrmRhThbVP/1KW1+winjvfXA54YzJe+qya5z7dwAWHj+CsW97k07U7qN5Rx4P/dURS95QJcxZ7jevHjunH7887iGvOnkhRQfu/Lx63v9dd/pqzJzTNFBBbxRQ7Df3PTjug1fOVurXSwwHvwju9mYcfdJ//eNq4dufV5I6VSExe2Oy++QMJJ2oMG9zT69V1XkXL7qFDekX3+OoRM7X4s1ccy8zj9mHSMK9dYczPnuXVJRuZMuslZs+rYmTf7k2r7L28eCNPLVjXNF5iZN9S7nprBYde8yL/de88Lrt/Ptc8vYiqrV6X481ZmuvrbxdOBrwqLX+ckkGqLjxyFDOPa67uqhjptX1ErjZY6qrPzjxoSKvnKfB5j5yP1myLKmGGpSOvJvusRGI6vcZgiKICHw0BbzGr/mVtB5IhvbrxwS9OijuNyoCyEo4d0483lm5i2oRBFBdEtx+UFPr56WkH8PiHVfzPIwtoDCqX3PV+0+c9igvo6c770mfVvOSmpy8t8ke1m4Qb9sFbUCrs21/Yh6tPbf1be3uFQorfJ3z7uH0y3i32/ksPp7YuQP+yYuav3sqDc1fzya9PIaiasFHc7/cCxV1vrWzx2UM5LMmZjrFAYjq11Zt3c+WjC5qCCCQXSKDlqOdI9804nLrGIMUJqn1OHj8IWNAiPdwTKdZZBw/hofe8uv5uhX5OnzS4aZGlI/bpw7vLvfGyTy9Yn5FAUl1bRzCkSf9+OqKk0E9JoReArz17Ir84fTw+n+CLO+FEM3+c7tIrfnsawZBat948ZoHEdFrzV23hK7e90/R+0rCefFy1PamqrWSEH4StKS0uYOX1p/Otu9/n5cUb+cqhw7j02NFNg/M++fXJ/P6FJZw7eTjjh5RTW9fIyRMGMXZgGUPcZIjfO3E/hvbq1vSQvPbpRdzx5gqW1+xkn5hBfh1R1xjktD+9AcCULDdY+3ySdK+w2KqrIr8PEaHAb1Va+cwCiem0Hvsgeprz2d8+kjVbdreYqC/T/nHJYVRurGVEn9KohuuykkJ+c3bzEKde3Ys4YWz0LLMj+0Y39p8+aTB3vLmCR+at6XCpRFW5/vnFbNhe1zQepl+PIsYPjj/+pTMoiAgkFx05kv/54v45zI1JFwskplP4YPVW9h9Y1tRVF7z1RMpKCvjbhZMZP7ickkJ/iwFw2bLfgPRc95ARvSkrKaC+MdT2zm34wSMfNQWQsPsvPbzTjrYHoroEf/Po0fS2SRW7BKuUNDn3eXUtX771bSb+6gXqA14X02BIeXrBekb27c5R+/ZrMWttPqutCzB73pq2d0wgEDEqH+DP5x/CK1cez7hBnbc0AtElklF9W5900+QXK5GYnFm0bgen3fxGVNotL1fyjSNGMuW6OQAcv3/XXJBod0OQP720lCum7teuEsSOOm9iyatOHZdwFHpnU1pcwD+/cyTjBpV16pKTSY0FEpMz/3XvvBZpN79cyc1uxDk0r3bYlVxw+AgenLuam176nNq6Rqpr63lqwTouPWY0NTvrufLksVFTi7xVuYmv3zGXy0/Yl0K/j/mrtvLGUm+KkfB4mXxy2Cgbvd7VyN4283pFRYXOm9fyAWay74K/v8vbyzZz7Jh+3HXJYdzw/GL+/kbzeIuFvzmlS865tGLTLk74/asJ95k8sjc/PmUsj86v4p/zq+Luc+DQnjzy7SNsKnWTFSIyX1Ur4n1m/wJNTjw6v4q3l23mtAMHcevXvVHYW3Y1r2b4o1PGdskgAjC6XymvXHk8l90/n8UbWq6HAjB/1Va+dvu7Te+P278/g8qL2X9gGV86ZCh9Sousash0Gl3zf6rJuZraem54fjGXHDWKiUObpzBfuWkXVVv3cOU/vYF+3z2+uerqS4cM5V8fVDHnh19osZBSVzO6Xyl/v6iCY298BYBXrjye0f1K2b6nkc+ra1GFP7+8lGG9u3P1aeMoL8lul2djUmFVWybtGoMhxvzsuai0iUPLmTC4J49E9FaaOm4Ad16S2oJLXVFjMGRrbZhOz6q2TNY8tWAd34uzlOqna3c0TWwY1tkWhcoVCyIm31kgMWmzrGZnVBB5/UcnsGj9dooKfCyv2cW1z3wGeGt+/Pz0A6yO35guwgKJ6TBV5f53V/GLJxYCcPDwXvz6rAmM6Nu9ab3tE8fB+MHl/OSxj5l53D4WRIzpQiyQGAA21tbRr7S4xap2O+sDvLhoAy8vruHtyk38eNpYvnzoMNZu3cPrS2vY3RDkxUXVzF+1FYAfnrQ/35s6Ju41jtqvH2/8+MSM34sxJrussb0Tq61rRESi5p/qiEAwxLY9jYRUaQiE2N0QZNG6HTz98Tpe+mwjJYU+LpgyksZgiPdXbmHxhloK/UJjMPG/kUHlJfzgi2M4r2K4LUxkTBfVpRvbRWQa8CfAD9yhqtdn+pp7GoJU76hjZ30AEW9q7MaAtw5EgV+aumrurA9QW9foLczk9+P3C6GQogqBkPcg31kfwCdCSBUB6gIhPl6zjcqanU1zKY3o052SQh8lhX4ag4qq0rt7EcN6d8PvE3w+YceeRhoCIQIhpTEYoiEQojEYojHorcq3fU8juxqCCe+rrjHEP95aQWmRnwMGlzN5ZG/GDSrjtAMHM3FoT174dAO/fPJTBpaX8I3DR3LCuAH071HctMiTMWbvlNclEhHxA58DJwFVwPvA+aq6qLVj2lsiefi91fzt9eVs3lnfNM9RJvUp9aYD31HXyOCeJQRDSiCkFPh8rNy8i7Vb99CreyHBkBJSpbS4gG6Ffgr9Pgr94l59FBX46NWtkN6lRfQoLqBvD28gW7HfR7ciP727F3HY6N4UF/hZuM5b62Ngef5Nu2GMyayuXCKZAlSq6nIAEXkYOBtoNZC018CeJYwdWMaAMf0YWF7CoPISykoKCKnX2CwCyzftwi9CQyCEAmUlBZSVFDYtExsMhfCJeD8+KC0qoEdxAUFV/CKEQ/q+/XswKMEcSuHgke5uoxOG9Gx7J2OMiZHvgWQoEDkfdxVweOxOIjITmAkwYsSIdl3ohLEDWixalCt+n+BvY0lTY4zJlnwfCRXvadqirk5Vb1fVClWt6N+/fxayZYwxe498DyRVwPCI98OAda3sa4wxJgPyPZC8D4wRkdEiUgRMB57McZ6MMWavktdtJKoaEJH/Bl7A6/77D1VdmONsGWPMXiWvAwmAqj4LPJvrfBhjzN4q36u2jDHG5JgFEmOMMR1igcQYY0yH5PUUKe0hIjXAqgycuh+wKQPnzZZ8zz/YPXQG+Z5/sHtozUhVjTsQb68LJJkiIvNam4cmH+R7/sHuoTPI9/yD3UN7WNWWMcaYDrFAYowxpkMskKTP7bnOQAfle/7B7qEzyPf8g91DyqyNxBhjTIdYicQYY0yHWCAxxhjTIRZIWiEiw0XkFRH5TEQWisj3XXofEXlRRJa6194uva/bf6eI/CXmXEUicruIfC4ii0XkK/mSfxEpE5GPIn42icgfM53/dN6D++x8EflERD4WkedFpF8e3sPXXP4XisiNnTT/J4nIfPe7ni8iJ0aca7JLrxSRm0UkK6uzpfkeZonIGhHZmY28p/seRKS7iDzjnkMLReT6tGRQVe0nzg8wGDjUbZfhrQ0/HrgRuMqlXwXc4LZLgWOA7wB/iTnXb4Br3bYP6JdP+Y8573zguHz6G+BNTrox/Ht3x/86z+6hL7Aa6O/e3wNM7YT5PwQY4rYnAmsjzvUecCTegnTPAad20r9Bons4wp1vZzbynu57ALoDJ7jtIuCNdPwdsvaLyPcf4AngJGAJMDjij7skZr9LaBlI1gCl+Zr/iM/GuHuRfLoHoBCoAUa6h9hfgZl5dg+HAS9FvL8QuLWz5t+lC7AZKHb7LI747Hzgb535bxB7DzHpWQ0kmbgH99mfgP/qaH6saisJIjIKL8LPBQaq6noA95pwIXcR6eU2rxGRD0TknyIyMIPZjZeHUbQz/zHOBx5R9y8wmzpyD6raCFwGfIK3guZ44M5M5jeeDv4dKoFxIjJKRAqAc4heHTTj2pH/rwAfqmo9MBRvRdOwKpeWVR28h04hXffgnk1nAnM6micLJG0QkR7Av4AfqOqOdpyiAG8J4LdU9VDgHeD3acxiQmnIf6TpwEMdz1VqOnoPIlKIF0gOAYYAHwNXpzWTbeehQ/egqlvx7uERvOqIlUAgnXlMJNX8i8gE4Abg2+GkOLtl9QtJGu4h59J1D+7LyEPAzaq6vKP5skCSgHsA/Qt4QFUfc8nVIjLYfT4Yr+49kc3AbuBx9/6fwKEZyG4Lacp/+FwHAQWqOj8jmW39uum4h4MBVHWZK03NBo7KTI5bStffQVWfUtXDVfVIvCqNpZnKc6RU8y8iw/D+vV+kqstcchXeF6qwYXilw6xI0z3kVJrv4XZgqar+MR15s0DSCtej5E7gM1X9Q8RHTwIXu+2L8eoqW+UeXE8Bx7ukqcCitGY2jnTlP8L5ZLk0ksZ7WAuMF5HwzKUnAZ+lM6+tSeffQUQGuNfewHeBO9Kb27jXTCn/rrrkGeBqVX0rvLOrdqkVkSPcOS8i+X97HZKue8ildN6DiFwL9AR+kLYM5rLBqDP/4PWcUbxqkI/cz2l4vWfm4H0bnAP0iThmJbAF2In3DWy8Sx8JvO7ONQcYkU/5d58tB8bl8d/gO3jB42O8wN43D+/hIbwvIYuA6Z0x/8DPgV0R+34EDHCfVQCfAsuAv5ClThtpvocb3d8k5F5/nU/3gFcSVPd/IZx+aUfzZ1OkGGOM6RCr2jLGGNMhFkiMMcZ0iAUSY4wxHWKBxBhjTIdYIDHGGNMhFkiMyTARCYo3c/JCEVkgIv8rIgn/77mpUC7IVh6N6QgLJMZk3h5VPVhVJ+ANhjwN+FUbx4wCLJCYvGDjSIzJMBHZqao9It7vA7wP9MMbrHof3vTxAP+tqm+LyLvAAcAKvCnjbwaux5shoRi4RVX/lrWbMCYBCyTGZFhsIHFpW4FxQC0QUtU6ERkDPKSqFSJyPHClqp7h9p+JN7r6WhEpBt4CzlPVFdm8F2PiKch1BozZS4Vnwy0E/iIiBwNBYP9W9j8ZmCQi57r3PfHWh7FAYnLOAokxWeaqtoJ4M7X+CqgGDsJrs6xr7TDge6r6QlYyaUwKrLHdmCxyMxD/FW/1Q8UrWaxX1RDeqod+t2st3pKqYS8Al7mpxBGR/UWkFGM6ASuRGJN53UTkI7xqrABe43p4KvBbgX+JyHnAK3gztoI3y2tARBYAd+MtiToK+MBNKV6Dt0qiMTlnje3GGGM6xKq2jDHGdIgFEmOMMR1igcQYY0yHWCAxxhjTIRZIjDHGdIgFEmOMMR1igcQYY0yH/H/DY4LM8WHQiQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading in, Viewing, and Refactoring Dataset\n",
    "df = pd.read_csv(r'../Datasets/BitcoinData.csv', index_col='Date', parse_dates=True) #read the csv file (put 'r' before the path string to address any special characters in the path, such as '\\'). Don't forget to put the file name at the end of the path + \".csv\"\n",
    "# print(df)\n",
    "# print(df.dtypes)\n",
    "cleanData = df\n",
    "\n",
    "repl_dict = {'[kK]': '*1e3', '[mM]': '*1e6', '[bB]': '*1e9', }\n",
    "cleanData['Vol.'] = cleanData['Vol.'].replace(repl_dict, regex=True).map(pd.eval).astype(float)\n",
    "\n",
    "cleanData['Price'] = cleanData['Price'].astype(str).str.replace(',', '').astype(float)\n",
    "cleanData['Open'] = cleanData['Open'].astype(str).str.replace(',', '').astype(float)\n",
    "cleanData['High'] = cleanData['High'].astype(str).str.replace(',', '').astype(float)\n",
    "cleanData['Low'] = cleanData['Low'].astype(str).str.replace(',', '').astype(float)\n",
    "cleanData['Change %'] = cleanData['Change %'].astype(str).str.replace('%', '').astype(float)\n",
    "# print(cleanData.describe())\n",
    "# print(cleanData.info())\n",
    "# print(cleanData.head())\n",
    "\n",
    "plt.plot(cleanData.Price)\n",
    "plt.title('Bitcoin Price Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95298a",
   "metadata": {},
   "source": [
    "## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efe6daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2285, 5) (2285,)\n"
     ]
    }
   ],
   "source": [
    "# Create X and y datasets\n",
    "X, y = cleanData.drop(columns=['Price']), cleanData.Price.values\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e30de9",
   "metadata": {},
   "source": [
    "### Training/Testing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a66677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Test Cutoff Samples: 1810\n",
      "Test Samples: 330\n",
      "Test-Train Samples: 240\n",
      "Test-Test Samples: 90\n",
      "Iteration 0\n",
      "Start set index: 200, End set index: 2170\n",
      "These should be different on iteration 2 and 3\n",
      "               Open     High      Low     Vol.  Change %\n",
      "Date                                                    \n",
      "2022-02-15  42550.3  43658.8  42460.1  56130.0      2.48\n",
      "2022-02-14  42061.1  42799.7  41591.3  53600.0      1.16\n",
      "2022-02-13  42205.5  42725.1  41880.1  25880.0     -0.34\n",
      "2022-02-12  42388.4  43006.4  41776.8  37880.0     -0.43\n",
      "2022-02-11  43519.3  43915.1  42023.4  69950.0     -2.60\n",
      "...             ...      ...      ...      ...       ...\n",
      "2015-11-19    335.9    335.6    323.4  86380.0     -3.26\n",
      "2015-11-18    333.9    337.4    329.4  78360.0      0.60\n",
      "2015-11-17    330.2    342.9    328.1  98250.0      1.12\n",
      "2015-11-16    317.5    332.0    314.1  87140.0      4.02\n",
      "2015-11-15    331.8    334.8    314.2  88320.0     -4.32\n",
      "\n",
      "[2285 rows x 5 columns]                Open     High      Low      Vol.  Change %\n",
      "Date                                                     \n",
      "2021-07-30  40001.1  42264.4  38358.4   98450.0      5.50\n",
      "2021-07-29  40009.0  40630.7  39340.8   75300.0     -0.00\n",
      "2021-07-28  39450.4  40862.2  38883.8  148920.0      1.40\n",
      "2021-07-27  37294.3  39455.9  36427.4  100240.0      5.84\n",
      "2021-07-26  35392.3  40522.9  35236.7  177630.0      5.33\n",
      "...             ...      ...      ...       ...       ...\n",
      "2016-03-13    410.4    415.9    409.6   34980.0      0.50\n",
      "2016-03-12    419.1    420.7    407.0   59640.0     -2.09\n",
      "2016-03-11    415.8    422.4    415.1   60630.0      0.79\n",
      "2016-03-10    412.8    417.5    410.3   55740.0      0.74\n",
      "2016-03-09    411.9    414.9    408.9   50920.0      0.21\n",
      "\n",
      "[1970 rows x 5 columns]\n",
      "(1970, 5) (1970, 1)\n",
      "(1642, 240, 5) (1642, 90)\n",
      "These should be the same....\n",
      "[0.30135096 0.29795336 0.29085561 0.3055453  0.28158625 0.27438237\n",
      " 0.26482156 0.26537436 0.29010006 0.29690477 0.28466072 0.28518818\n",
      " 0.289533   0.28934292 0.2755355  0.27507773 0.27329894 0.25830671\n",
      " 0.24622422 0.2480933  0.2521039  0.25163663 0.24215026 0.23593795\n",
      " 0.23631177 0.23879067 0.22841095 0.24028277 0.24042849 0.21759564\n",
      " 0.21560459 0.20834211 0.21147836 0.21207869 0.20831835 0.20669162\n",
      " 0.20386741 0.20986905 0.20042545 0.19995977 0.20130614 0.1984059\n",
      " 0.1990474  0.19641961 0.18223995 0.17970402 0.17579955 0.17350596\n",
      " 0.17287079 0.17573777 0.17442942 0.17448327 0.17622722 0.17364694\n",
      " 0.17249698 0.16862894 0.1665682  0.1625576  0.16147575 0.16443618\n",
      " 0.16258928 0.16055071 0.16099581 0.16175928 0.16422393 0.16525034\n",
      " 0.16291082 0.16422551 0.16346046 0.16284113 0.16364261 0.15568952\n",
      " 0.16034955 0.15853274 0.16652701 0.16906611 0.16670917 0.16684064\n",
      " 0.16697052 0.16436965 0.16262729 0.1570945  0.15893032 0.15811141\n",
      " 0.1573115  0.15548836 0.15393607 0.15790074 0.15662565 0.15339119]\n",
      "[0.30135096 0.29795336 0.29085561 0.3055453  0.28158625 0.27438237\n",
      " 0.26482156 0.26537436 0.29010006 0.29690477 0.28466072 0.28518818\n",
      " 0.289533   0.28934292 0.2755355  0.27507773 0.27329894 0.25830671\n",
      " 0.24622422 0.2480933  0.2521039  0.25163663 0.24215026 0.23593795\n",
      " 0.23631177 0.23879067 0.22841095 0.24028277 0.24042849 0.21759564\n",
      " 0.21560459 0.20834211 0.21147836 0.21207869 0.20831835 0.20669162\n",
      " 0.20386741 0.20986905 0.20042545 0.19995977 0.20130614 0.1984059\n",
      " 0.1990474  0.19641961 0.18223995 0.17970402 0.17579955 0.17350596\n",
      " 0.17287079 0.17573777 0.17442942 0.17448327 0.17622722 0.17364694\n",
      " 0.17249698 0.16862894 0.1665682  0.1625576  0.16147575 0.16443618\n",
      " 0.16258928 0.16055071 0.16099581 0.16175928 0.16422393 0.16525034\n",
      " 0.16291082 0.16422551 0.16346046 0.16284113 0.16364261 0.15568952\n",
      " 0.16034955 0.15853274 0.16652701 0.16906611 0.16670917 0.16684064\n",
      " 0.16697052 0.16436965 0.16262729 0.1570945  0.15893032 0.15811141\n",
      " 0.1573115  0.15548836 0.15393607 0.15790074 0.15662565 0.15339119]\n",
      "Train size: 1810 Test Size: 330\n",
      "Training Shape: (1312, 240, 5) (1312, 90)\n",
      "Testing Shape: (330, 240, 5) (330, 90)\n",
      "--------------------------------------------------\n",
      "TASHMAN TRAINING SPLIT\n",
      "The first training set is from 0 to 1050\n",
      "The second training set is from 328 to 1247\n",
      "The third training set is from 525 to 1312\n",
      "--------------------------------------------------\n",
      "TASHMAN TESTING SPLIT\n",
      "The tashman test split is 30\n",
      "--------------------------------------------------\n",
      "Training Shape: torch.Size([1312, 240, 5]) torch.Size([1312, 90])\n",
      "Testing Shape: torch.Size([330, 240, 5]) torch.Size([330, 90])\n",
      "             Open   High    Low     Vol.  Change %\n",
      "Date                                              \n",
      "2016-10-08  616.0  619.6  614.9  13620.0      0.26\n",
      "2016-10-07  611.0  619.4  609.1  32730.0      0.83\n",
      "2016-10-06  611.8  612.7  608.5  21630.0     -0.14\n",
      "These should be the same....\n",
      "[384.  379.5 381.9 375.3 372.6 377.9 376.7 386.5 390.6 368.  373.9 372.2\n",
      " 369.8 377.8 380.3 379.7 395.  389.8 392.8 402.1 388.6 382.6 410.2 414.6\n",
      " 379.5 384.4 382.5 385.  372.3 429.1 432.2 445.  447.7 446.2 448.3 452.9\n",
      " 457.  430.8 431.2 433.3 430.7 433.7 434.  430.  427.1 431.9 421.8 422.4\n",
      " 415.4 454.  453.  442.4 437.  437.6 441.8 461.2 463.2 455.5 454.  462.6\n",
      " 442.  434.7 432.3 449.8 415.5 416.  410.7 394.3 393.4 386.7 361.7 360.3\n",
      " 360.  361.8 378.  370.8 355.8 359.5 353.7 327.5 318.4 322.1 322.8 324.7\n",
      " 321.1 325.  335.9 333.9 330.2 317.5]\n",
      "[384.  379.5 381.9 375.3 372.6 377.9 376.7 386.5 390.6 368.  373.9 372.2\n",
      " 369.8 377.8 380.3 379.7 395.  389.8 392.8 402.1 388.6 382.6 410.2 414.6\n",
      " 379.5 384.4 382.5 385.  372.3 429.1 432.2 445.  447.7 446.2 448.3 452.9\n",
      " 457.  430.8 431.2 433.3 430.7 433.7 434.  430.  427.1 431.9 421.8 422.4\n",
      " 415.4 454.  453.  442.4 437.  437.6 441.8 461.2 463.2 455.5 454.  462.6\n",
      " 442.  434.7 432.3 449.8 415.5 416.  410.7 394.3 393.4 386.7 361.7 360.3\n",
      " 360.  361.8 378.  370.8 355.8 359.5 353.7 327.5 318.4 322.1 322.8 324.7\n",
      " 321.1 325.  335.9 333.9 330.2 317.5]\n",
      "First tashman split training\n",
      "Epoch: 0, train loss: 0.04975, test loss: 0.02965\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/6w/t50_lm0d33j1xdw4t89klv100000gn/T/ipykernel_27378/3761819570.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# START FIRST TASHMAN SPLIT TRAINING HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'First tashman split training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     training_loop(n_epochs=n_epochs,\n\u001b[0m\u001b[1;32m    159\u001b[0m                   \u001b[0mlstm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                   \u001b[0moptimiser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6w/t50_lm0d33j1xdw4t89klv100000gn/T/ipykernel_27378/2740091316.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, lstm, optimiser, loss_fn, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# calculate the gradient, manually setting to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# obtain the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/6w/t50_lm0d33j1xdw4t89klv100000gn/T/ipykernel_27378/2740091316.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mc_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# propagate input through LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (input, hidden, and internal state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reshaping the data for Dense layer next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    692\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We want to predict data several months or so in the future, here we will set the training data size at 85%\n",
    "# and the remaining 15% can be used for testing data\n",
    "train_test_percentage = 0.85\n",
    "test_test_percentage = 0.70\n",
    "total_samples = 2200 # len(X)\n",
    "train_test_cutoff = 1810 # round(train_test_percentage * total_samples) # TRAIN/TEST split cutoff!!!!!!!!!\n",
    "test_samples = 330 # total_samples - train_test_cutoff\n",
    "train_test_size = 240 # round(test_test_percentage * test_samples)\n",
    "test_test_size = 90 # test_samples - train_test_size\n",
    "print(f'Train-Test Cutoff Samples: {train_test_cutoff}')\n",
    "print(f'Test Samples: {test_samples}')\n",
    "print(f'Test-Train Samples: {train_test_size}')\n",
    "print(f'Test-Test Samples: {test_test_size}')\n",
    "\n",
    "\n",
    "# Time to instantiate an instance of our LSTM model\n",
    "n_epochs = 1000 # 1000 epochs\n",
    "learning_rate = 0.001 # 0.001 lr\n",
    "\n",
    "input_size = 5 # number of features\n",
    "hidden_size = 2 # number of features in hidden state\n",
    "num_layers = 1 # number of stacked lstm layers\n",
    "\n",
    "# num_classes = test_test_size # number of output classes (Amount of data we are predicting outwards)\n",
    "num_classes = 90 # This needs to match the tashman test split\n",
    "\n",
    "# Set up the 3 training periods\n",
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n",
    "\n",
    "# Here we will use MSE (Mean Squared Error) as our loss function, this can be changed to an absolute error to be\n",
    "# in accordance with Tashman method\n",
    "loss_fn = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimiser = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "mm = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Splitting the original set for tashman\n",
    "first_end_orig_set = total_samples - 60\n",
    "# first split will go from 0 to where it is now\n",
    "first_start_orig_set = 200\n",
    "second_end_orig_set = total_samples - 30\n",
    "second_start_orig_set = 550\n",
    "third_end_orig_set = total_samples\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'Iteration {i}')\n",
    "    start = 0\n",
    "    end = 0\n",
    "    if i == 0:\n",
    "        end = first_end_orig_set\n",
    "    elif i == 1:\n",
    "        start = first_start_orig_set\n",
    "        end = second_end_orig_set\n",
    "    elif i == 2:\n",
    "        start = second_start_orig_set\n",
    "        end = third_end_orig_set\n",
    "    else:\n",
    "        print('Big problem')\n",
    "        exit(1)\n",
    "        \n",
    "    print(f'Start set index: {start}, End set index: {end}')\n",
    "    print('These should be different on iteration 2 and 3')\n",
    "    print(X, X[start:end])\n",
    "    X_trans = ss.fit_transform(X[start:end])\n",
    "    y_trans = mm.fit_transform(y[start:end].reshape(-1, 1))\n",
    "    print(X_trans.shape, y_trans.shape)\n",
    "\n",
    "    X_ss, y_mm = split_sequences(X_trans, y_trans, train_test_size, test_test_size)\n",
    "    print(X_ss.shape, y_mm.shape)\n",
    "\n",
    "    print('These should be the same....')\n",
    "    print(y_mm[0])\n",
    "    print(y_trans[(train_test_size-1):(test_samples-1)].squeeze(1))\n",
    "\n",
    "    assert y_mm[0].all() == y_trans[train_test_size:test_samples-1].squeeze(1).all()\n",
    "\n",
    "    print(f'Train size: {train_test_cutoff} Test Size: {test_samples}')\n",
    "\n",
    "    X_train = X_ss[:(-1*test_samples)]\n",
    "    X_test = X_ss[(-1*test_samples):]\n",
    "\n",
    "    y_train = y_mm[:(-1*test_samples)]\n",
    "    y_test = y_mm[(-1*test_samples):]\n",
    "\n",
    "    print(\"Training Shape:\", X_train.shape, y_train.shape)\n",
    "    print(\"Testing Shape:\", X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "    # HERE IS WHERE TASHMAN WILL BE CONDUCTED\n",
    "    print('-'*50)\n",
    "    print('TASHMAN TRAINING SPLIT')\n",
    "    total_train_samples = X_train.shape[0]\n",
    "    tashman_split = .8\n",
    "    percentage_push = .25\n",
    "    percentage_push_2 = .4\n",
    "    middle_end_percentage = .15\n",
    "    second_start_limit = round(percentage_push*total_train_samples)\n",
    "    third_start_limit = round(percentage_push_2*total_train_samples)\n",
    "    start_limit = round(tashman_split*total_train_samples)\n",
    "    second_limit = round(middle_end_percentage*total_train_samples)\n",
    "    print(f'The first training set is from 0 to {start_limit}')\n",
    "    print(f'The second training set is from {second_start_limit} to {start_limit+second_limit}')\n",
    "    print(f'The third training set is from {third_start_limit} to {total_train_samples}')\n",
    "    print('-'*50)\n",
    "    print('TASHMAN TESTING SPLIT')\n",
    "    # Split by 1/3\n",
    "    test_split_limit = round((1/3)*test_test_size)\n",
    "    print(f'The tashman test split is {test_split_limit}')\n",
    "    print('-'*50)\n",
    "\n",
    "\n",
    "    # Now we need to convert our test and training sets into tensors, which are like numpy arrays but are used\n",
    "    # in pytorch\n",
    "    X_train_tensors = Variable(torch.Tensor(X_train))\n",
    "\n",
    "    X_test_tensors = Variable(torch.Tensor(X_test))\n",
    "\n",
    "    y_train_tensors = Variable(torch.Tensor(y_train))\n",
    "    y_test_tensors = Variable(torch.Tensor(y_test))\n",
    "\n",
    "    # Here we will use the LSTM NN for our prediction algorithm. The input shape of the data depends on whether\n",
    "    # batch_first is true or not. Here, we will set it to be true, meaning the size of the input is (N, L, Hin),\n",
    "    # where N is the batch size, L is the sequence length, and Hin is the input size (number of features). In other\n",
    "    # words, we want the dimensions to be the rows of the dataframe in the first dimension, followed by the length of\n",
    "    # the dataframe in the second dimension, and finally the features (in which we have 5) in the final dimension.\n",
    "    X_train_tensors_final = torch.reshape(X_train_tensors,   \n",
    "                                          (X_train_tensors.shape[0], X_train_tensors.shape[1], \n",
    "                                           X_train_tensors.shape[2]))\n",
    "    X_test_tensors_final = torch.reshape(X_test_tensors,  \n",
    "                                         (X_test_tensors.shape[0], X_test_tensors.shape[1], \n",
    "                                          X_test_tensors.shape[2]))\n",
    "\n",
    "    print(\"Training Shape:\", X_train_tensors_final.shape, y_train_tensors.shape)\n",
    "    print(\"Testing Shape:\", X_test_tensors_final.shape, y_test_tensors.shape)\n",
    "\n",
    "    # Lastly, we want to check the data logic of the test set. We want to predict 50 timesteps into the future with \n",
    "    # Bitcoin. Based on split_sequence() above, we simply need to add the last sample of 100 days to X_test, run the \n",
    "    # model on it, and compare these predictions with the last 50 days of y_test, these correspond to a period of 100\n",
    "    # days in X_test's last sample, proceeded immediately by the next 50 days in the last sample of y_test\n",
    "    # HERE, we want to check that the last 50 samples that we will be predicting match the last 50 values of y \n",
    "    # in the test set\n",
    "    X_check, y_check = split_sequences(X, y.reshape(-1, 1), train_test_size, test_test_size)\n",
    "    X_check[-1][0:4]\n",
    "\n",
    "    print(X.iloc[(-1*(test_samples-1)):(-1*(test_samples-4))])\n",
    "\n",
    "    print('These should be the same....')\n",
    "    print(y_check[-1])\n",
    "    print(cleanData.Price.values[(-1*test_test_size):])\n",
    "    \n",
    "    # Train here\n",
    "    # Training lstm_first_class\n",
    "    # START FIRST TASHMAN SPLIT TRAINING HERE\n",
    "    print('First tashman split training')\n",
    "    training_loop(n_epochs=n_epochs,\n",
    "                  lstm=lstm,\n",
    "                  optimiser=optimiser,\n",
    "                  loss_fn=loss_fn,\n",
    "                  X_train=X_train_tensors_final,\n",
    "                  y_train=y_train_tensors,\n",
    "                  X_test=X_test_tensors_final,\n",
    "                  y_test=y_test_tensors)\n",
    "    print('First tashman split testing')\n",
    "    tashman_testing(lstm=lstm, \n",
    "                    loss_fn=loss_fn, \n",
    "                    X_test=X_test_tensors_final, \n",
    "                    y_test=y_test_tensors, \n",
    "                    mm=mm)\n",
    "    print('Second tashman split testing')\n",
    "    tashman_testing(lstm=lstm, \n",
    "                    loss_fn=loss_fn, \n",
    "                    X_test=X_test_tensors_final, \n",
    "                    y_test=y_test_tensors, \n",
    "                    mm=mm)\n",
    "    print('Third tashman split testing')\n",
    "    tashman_testing(lstm=lstm, \n",
    "                    loss_fn=loss_fn, \n",
    "                    X_test=X_test_tensors_final, \n",
    "                    y_test=y_test_tensors, \n",
    "                    mm=mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d09843d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
